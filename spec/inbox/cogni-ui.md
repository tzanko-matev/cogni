# Designing Cogni's User Interface and Integration Patterns

## Language-Agnostic Support and Initial Language Focus

Cogni is intended to be **language-agnostic**, similar to tools like coala which provide a unified interface for many languages[\[1\]](https://coala.github.io/website-old/#:~:text=coala%20provides%20a%20user%20and,and%20users%20focus%20on%20content). This means the core system should be flexible enough to analyze any codebase regardless of language. In practice, many multi-language tools start by supporting the most popular languages first. For example, surveys show that JavaScript and Python consistently rank among the top-used programming languages[\[2\]](https://www.stackscale.com/blog/most-popular-programming-languages/#:~:text=Position%20PYPL%20ranking%20September%202023,R%20Java%208%20TypeScript%20C), along with others like Java, C/C++, and C#. Focusing initial support on these popular languages ensures Cogni delivers value to the largest user base. Coala's approach is instructive - it **supports dozens of languages out of the box** by having language-specific "bears" (analysis plugins) but one common CLI interface[\[1\]](https://coala.github.io/website-old/#:~:text=coala%20provides%20a%20user%20and,and%20users%20focus%20on%20content). Cogni can adopt a similar plugin-based architecture, enabling new languages to be added over time without changing the core UI.

## Command-Line Interface (CLI) Patterns

As a Go binary, Cogni will primarily be a **CLI tool**, so a well-designed CLI UX is critical. Successful developer tools follow certain patterns to make the CLI intuitive:

- **Task-Oriented Commands:** Use verbs and nouns that match user goals. For example, coverage.py uses subcommands like run (to collect data) and report/html (to generate output)[\[3\]](https://coverage.readthedocs.io/en/7.13.1/#:~:text=3.%20Use%20,report%20on%20the%20results)[\[4\]](https://coverage.readthedocs.io/en/7.13.1/#:~:text=4,HTML%20listings%20detailing%20missed%20lines). Cogni could have commands such as cogni run (to execute the Q&A on a codebase) and cogni report (to output results). Keeping commands high-level ("tasks, not APIs") improves usability[\[5\]](https://medium.com/@kaushalsinh73/10-cli-ux-patterns-that-users-will-brag-about-015e4d0c268d#:~:text=1,APIs).
- **Simple Default, Advanced Flags:** Provide a one-command happy path with sensible defaults, but allow detailed tuning via flags. For instance, running coverage run with minimal flags collects data, while advanced options (--source, --omit, etc.) are there if needed[\[6\]](https://coverage.readthedocs.io/en/7.13.1/#:~:text=To%20limit%20coverage%20measurement%20to,measure%20%20or%20%2030). Cogni might default to running a standard question set against the whole repo, but offer flags for custom question files, target directories, or agent parameters.
- **Human-Readable vs Machine-Readable Output:** A common pattern is _human-friendly output by default, with a machine format option_. "Humans like tables. Scripts like JSON. Give both, consistently."[\[7\]](https://medium.com/@kaushalsinh73/10-cli-ux-patterns-that-users-will-brag-about-015e4d0c268d#:~:text=7%29%20Structured%20Output%2C%20Stable%20Tables,by%20default%20acme%20services) Many CLI tools (e.g., Kubernetes' kubectl, Google Benchmark) print results in a nicely formatted table or text for people, and offer a --json or --format flag for JSON or CSV output for automation[\[8\]](https://bencher.dev/learn/track-in-ci/cpp/google-benchmark/#:~:text=The%20C%2B%2B%20Google%20Adapter%20%28,are%20collected). Cogni should do the same - e.g. summary statistics and highlights in plain text, but --output json to export detailed metrics in JSON for CI pipelines or further analysis[\[7\]](https://medium.com/@kaushalsinh73/10-cli-ux-patterns-that-users-will-brag-about-015e4d0c268d#:~:text=7%29%20Structured%20Output%2C%20Stable%20Tables,by%20default%20acme%20services).
- **Structured, Colorful Output:** When outputting to the terminal, **use formatting to improve readability**. Testing tools often color-code results (green for pass, red for fail), and linters or profilers use columns or tables for data. For example, coverage.py's text report uses aligned columns for file coverage percentages[\[3\]](https://coverage.readthedocs.io/en/7.13.1/#:~:text=3.%20Use%20,report%20on%20the%20results). Tools like Yarn or Docker employ color and symbols to highlight warnings vs. successes[\[9\]](https://www.lucasfcosta.com/blog/ux-patterns-cli-tools#:~:text=When%20the%20output%20is%20displayed,went%20okay%20despite%20the%20warning)[\[10\]](https://www.lucasfcosta.com/blog/ux-patterns-cli-tools#:~:text=1,users%20may%20want%20to%20search). Cogni can adopt similar techniques - perhaps green text or a "✅" for questions the AI answered easily, yellow for moderate effort, red for high effort - to give at-a-glance feedback to developers. Just ensure the output remains grep-able and not overly noisy[\[11\]](https://www.lucasfcosta.com/blog/ux-patterns-cli-tools#:~:text=1,users%20may%20want%20to%20search) (important for parsing logs).
- **Progress Indicators:** If Cogni's analysis might take significant time (for example, if the AI is processing a large codebase or many questions), provide feedback during execution. Many CLI tools show progress bars or activity spinners so users (and CI systems) know the tool hasn't hung. For instance, Docker prints progress for each image layer pull[\[12\]](https://www.lucasfcosta.com/blog/ux-patterns-cli-tools#:~:text=One%20of%20my%20favourite%20progress,indicators%20is%20Docker%27s). Cogni might show progress per question or a percentage of files processed, preventing confusion during long runs.

Finally, **exit codes** should be used meaningfully. In CI, exit code 0 means success, non-zero indicates failure[\[13\]](https://www.lucasfcosta.com/blog/ux-patterns-cli-tools#:~:text=Exit%20codes). Cogni might not have "pass/fail" in the traditional sense, but if you implement thresholds (e.g. fail the build if a complexity metric exceeds a limit), use exit codes accordingly. For example, some code quality tools let you configure _fail conditions_ (like coverage drop or linter errors); Cogni could allow an optional threshold where if cognitive complexity regresses beyond a point, it exits with an error code to alert CI.

## Configuration Management (Questions & Agent Settings)

Cogni's uniqueness is that it asks AI agents questions about the code. Deciding **how to manage these questions and AI settings** is crucial:

- **Question Sets Definition:** The questions could be managed via configuration files. A good parallel is how test suites or linters manage their rules. Tools like coala allow saving configuration to a file (e.g., .coafile) which includes what checks to run[\[14\]](https://coala.github.io/website-old/#:~:text=%60cd%20project%20%26%26%20coala%20,save). Cogni could use a YAML or JSON config (e.g., cogni.yaml) listing the questions to ask and any expected outcomes or weights. This file would live in the repo so it can evolve with the codebase (much like a test config or a pytest.ini). In fact, coala even provides a --save flag that auto-writes your settings to a config file for future runs[\[14\]](https://coala.github.io/website-old/#:~:text=%60cd%20project%20%26%26%20coala%20,save). Cogni might offer a similar interactive setup (e.g., a wizard to select default question templates, then save to config).
- **AI Agent Settings:** Since different AI models ("agents") might be used, their credentials and parameters need configuring. **Sensitive info** like API keys are often set via environment variables (e.g., OPENAI_API_KEY) rather than stored in config files, for security. Other settings like which model to use or max tokens could be flags or config fields. For example, you might allow a config section:
- agent:  
    provider: openai  
    model: gpt-4  
    max_tokens: 2000
- with an override via CLI flag like --model gpt-3.5-turbo. This follows typical patterns: **defaults in a config file, overrideable by CLI flags**. Many tools (like linters or formatters) use this layered config approach. A concrete example is how **Super-Linter** is configured - it uses environment variables to select which linters to run and can detect config files for each linter[\[15\]](https://thedocumentation.org/super-linter/usage/configuration/#:~:text=Configuration%20via%20Environment%20Variables)[\[16\]](https://thedocumentation.org/super-linter/usage/configuration/#:~:text=The%20%60VALIDATE_,specific%20logic). Cogni could similarly have an environment-based config for CI (easy to tweak in GitHub Actions YAML) and/or a file for more complex setups.
- **Profiles for Different Stakeholders:** If needed, Cogni could support _profiles_ or modes (similar to how some CI tools have "dev" vs "ci" modes). For instance, a **"fast mode"** with fewer questions for quick local runs, versus a **"full audit"** mode with extensive questioning for nightly builds. This can be handled via config or subcommands (e.g., cogni run --profile quick). The Medium article on CLI UX patterns suggests using profiles when appropriate to bundle sets of options (e.g., a user could configure a profile for an "AI thoroughness level")[\[17\]](https://medium.com/@kaushalsinh73/10-cli-ux-patterns-that-users-will-brag-about-015e4d0c268d#:~:text=Ten%20CLI%20UX%20patterns%20%E2%80%94,%E2%80%94%20that%20make%20users%20rave).

## Continuous Integration Integration (GitHub Actions & Beyond)

Integrating Cogni into CI/CD pipelines is a top priority. Here are patterns drawn from other tools' CI integration:

- **Straightforward CLI Execution:** In many cases, integrating is as simple as running the tool's CLI in the pipeline. For example, projects run tests with pytest or run coverage with coverage run && coverage report directly in a CI YAML. Cogni should be easy to run in a similar way. A GitHub Actions step might be:  

- \- name: Run Cogni Cognitive Benchmark  
    run: cogni run --output json > cogni_results.json
- ensuring the CLI can run in non-interactive mode (no prompts) and produce a deterministic output. Coala explicitly has a --ci flag for this purpose, which disables interactive prompts and ensures consistent formatting for CI[\[18\]](https://coala.github.io/website-old/#:~:text=Just%20Execute%20). Cogni might include a similar switch if needed (though ideally just running it should be CI-friendly by default).
- **Official GitHub Action**: To lower friction, you can provide a pre-built GitHub Action (e.g., uses: cogni/cogni-action@v1). Many open-source tools do this to handle setup. For instance, Codecov offers an action to upload coverage results easily[\[19\]](https://github.com/marketplace/actions/codecov#:~:text=Codecov%20%C2%B7%20Actions%20%C2%B7%20GitHub,Wrapper%20to%20encapsulate%20the%20CLI). A Cogni action could handle installing the Cogni binary (or use a Docker container) and then running it with given inputs. This allows users (especially non-Go ecosystems) to drop Cogni into their workflow without manual installation.
- **CI Outputs and Artifacts**: Decide how Cogni communicates results in CI. Common practices include:
- **Failing the build on certain conditions**: e.g., if the cognitive complexity score passes a threshold. This uses exit codes as mentioned. Not every run needs to fail the pipeline - often, these tools run in informational mode (e.g., posting a comment) unless a gate is configured.
- **Uploading artifacts**: If Cogni produces a detailed report (JSON, HTML, etc.), the CI job can upload it. Developers or managers can then download the artifact from the CI run. For example, test frameworks often output JUnit XML which CI surfaces, and coverage tools produce an HTML report artifact for download.
- **PR comments or statuses**: Some tools (like linters or Codecov) will post a summary on the Pull Request. Cogni could similarly post, say, "Cognitive complexity score: 7.5/10 (increased from 6.8 last week)" on a PR, to make it visible to developers and reviewers. This requires either a GitHub App or using the GitHub Actions API to add a PR comment. Initially, focusing on artifacts and console output is simpler, but keeping this pattern in mind is useful as the tool evolves.
- **Multi-Platform CI**: Since other CI systems (GitLab CI, Jenkins, etc.) may be used, ensure Cogni can run in a Docker container or as a simple downloadable binary. Dockerizing Cogni (like docker run cognicorp/cogni:latest ...) is one way to ease adoption in varied environments, much like how coala provided a Docker run one-liner for convenience[\[20\]](https://coala.github.io/website-old/#:~:text=Alternative%3A%20Run%20it%20in%20Docker). This also helps in CI systems that can easily pull containers.

## Output and Reporting Formats

The results of Cogni's analysis need to be consumable by different audiences - developers, DevOps, and managers - so multiple output formats are beneficial:

- **Console Summary:** A short textual summary for immediate feedback. For example, Cogni might output something like: _"Average AI effort: 120 tokens/question (↑10 from last run), 2 questions failed to get clear answers."_ This gives developers a quick sense of the code's state right in the CI log or local terminal.
- **Detailed Machine-Readable Data:** Cogni should export detailed metrics in a format that can be parsed or imported elsewhere. **JSON is a good choice** due to its ubiquity. Many tools support JSON output for this reason - e.g. Google Benchmark allows --benchmark_out_format=json to emit all benchmark measurements in JSON[\[21\]](https://stackoverflow.com/questions/46117069/how-to-force-black-and-white-output-in-google-benchmark#:~:text=The%20format%20of%20the%20output,does%20not%20suppress%20the). In fact, continuous benchmarking systems like **Bencher** expect JSON results from Google Benchmark to ingest performance metrics[\[8\]](https://bencher.dev/learn/track-in-ci/cpp/google-benchmark/#:~:text=The%20C%2B%2B%20Google%20Adapter%20%28,are%20collected). Cogni's JSON could include each question asked, the AI's response time or token usage, whether it answered correctly (if an answer key is known), etc. This data can then feed into dashboards or be versioned over time.
- **HTML/Visual Reports:** For manager-level consumption or deep analysis, consider generating an **HTML report** or other visual format. Code coverage tools set a precedent here: they often produce an interactive HTML site that developers can open to explore coverage file by file. Coverage.py, for example, generates an htmlcov/index.html which shows coverage percentages in a user-friendly way[\[4\]](https://coverage.readthedocs.io/en/7.13.1/#:~:text=4,HTML%20listings%20detailing%20missed%20lines). Cogni could create a static HTML report with charts - e.g., a trend line of "cognitive complexity score" over time, a list of the hardest-to-understand modules, etc. This could be as simple as an HTML file with embedded JavaScript (like Allure reports for test results, or a D3.js chart). The key is that a non-developer (or anyone without running the CLI) can open this report and grasp the insights. This report could be uploaded as a CI artifact or even hosted via GitHub Pages or an internal dashboard.
- **Standardized Formats:** To integrate with existing tools, it might be useful to **optionally output in standard formats**. For instance, JUnit XML format is recognized by many CI systems for test results. If Cogni frames each question as a "test" (pass if answered under some effort threshold, fail if not), it could emit a JUnit-style XML - which would allow CI systems to show it in the tests panel. Similarly, Cogni could output a summary metric that can feed into a badge (like Codecov and others provide badges for coverage). While not mandatory, these integrations can help slot Cogni into existing DevOps workflows with minimal friction.

## Web-Based Dashboards and Trend Tracking

Over time, one of Cogni's goals is to **track metrics to measure technical debt**. This implies storing historical data and showing trends. Patterns from tools like SonarQube and continuous benchmarking platforms can guide this:

- **Central Dashboards:** SonarQube (open-source) is a well-known tool that collects code quality metrics (coverage, duplication, complexity, tech debt) and displays them on a web dashboard. Typically, a CI job runs an analysis and pushes the data to a server. The web UI then allows developers and managers to see **trend charts**, drill down into specific issues, and even compare branches. While building a full SonarQube-like interface might be beyond Cogni's initial scope, using a **lightweight web UI for trends is powerful**. For example, **Bencher** (the continuous benchmarking suite) provides a web console where you can query and graph performance results over time[\[22\]](https://bencher.dev/learn/track-in-ci/cpp/google-benchmark/#:~:text=,they%20make%20it%20to%20production). Cogni could follow a similar approach by sending its results to a service or by encouraging users to feed the JSON output into existing visualization tools.
- **Integration with Existing Analytics:** To avoid reinventing the wheel, Cogni's export could be made compatible with tools like **Code Climate** or **CodeMetric** services if they exist for AI metrics. In absence of a dedicated service, users might dump Cogni's data into a time-series database or even a spreadsheet. Thus, **document the schema** of the JSON/CSV output clearly. This allows DevOps to script custom solutions (e.g., pushing data to InfluxDB and using Grafana to plot "Cogni score over time"). The design principle here is _openness_: since Cogni is open-source, providing open data formats means communities can build their own small UIs or analyses on top of it.
- **Role-Based Views:** Different stakeholders care about different things. **Developers** will want to know _"Which parts of the code confused the AI and might confuse new contributors?"_ while **managers** might care about high-level indices (e.g., a single "cognitive complexity" number for the project that correlates with maintainability). Consider presenting multiple levels of detail. A pattern from test coverage tools: they show an overall percentage (one number) for quick assessment, but also allow drilling down by module/file. Cogni could compute an overall _score_ or _grade_ for the codebase's understandability, and also provide per-module or per-question details for those who want to dive in. A web report could have a top section with overall metrics and trend graphs, and lower sections with detailed Q&A results.

Finally, keep the **user interface consistent and accessible**. Users should not need to fight the tool to get information out. By mirroring patterns from familiar tools (tests, linters, coverage), Cogni will fit naturally into the development process. For example, just as a developer might run tests locally before pushing, they could run Cogni locally to see if the AI struggles with their new code. And just as coverage reports are reviewed for every release, Cogni's reports could become a routine checkpoint - provided they are easy to generate and read.

## Conclusion

In summary, **Cogni's interface should borrow the best practices from existing developer tools** to ensure adoption:

- _CLI Design:_ Intuitive subcommands and flags, human-readable default output with optional JSON export[\[7\]](https://medium.com/@kaushalsinh73/10-cli-ux-patterns-that-users-will-brag-about-015e4d0c268d#:~:text=7%29%20Structured%20Output%2C%20Stable%20Tables,by%20default%20acme%20services), use of color/tables for clarity, and proper exit codes for CI[\[13\]](https://www.lucasfcosta.com/blog/ux-patterns-cli-tools#:~:text=Exit%20codes).
- _Config and Extensibility:_ A configuration file (YAML/JSON or similar) to manage what questions are asked and which AI agent is used, much like how linting rules or test settings are configured. Support overriding via CLI/env for CI convenience. Keep it language-agnostic by design (plugins per language)[\[1\]](https://coala.github.io/website-old/#:~:text=coala%20provides%20a%20user%20and,and%20users%20focus%20on%20content), focusing first on widely-used languages[\[2\]](https://www.stackscale.com/blog/most-popular-programming-languages/#:~:text=Position%20PYPL%20ranking%20September%202023,R%20Java%208%20TypeScript%20C).
- _CI Integration:_ Make it drop-in for GitHub Actions (and extendable to others) - non-interactive mode[\[18\]](https://coala.github.io/website-old/#:~:text=Just%20Execute%20), straightforward installation or official action, and producing outputs (logs, artifacts, exit status) that CI systems can work with. Possibly provide threshold settings to fail CI when complexity worsens beyond a point (similar to a quality gate).
- _Output Formats:_ Provide multi-format output - concise text summary, rich HTML report for deeper analysis[\[4\]](https://coverage.readthedocs.io/en/7.13.1/#:~:text=4,HTML%20listings%20detailing%20missed%20lines), and structured data (JSON/CSV) for tracking metrics over time[\[23\]](https://coverage.readthedocs.io/en/7.13.1/#:~:text=,what%20tests%20ran%20which%20lines)[\[8\]](https://bencher.dev/learn/track-in-ci/cpp/google-benchmark/#:~:text=The%20C%2B%2B%20Google%20Adapter%20%28,are%20collected). This caters to developers running quick checks, as well as managers looking at longer-term trends.
- _Visualization and Trends:_ Emulate the dashboard approach of tools like SonarQube or Bencher by enabling trend analysis. Even if Cogni doesn't ship with a full web service, the data it produces can be used to plot graphs of "cognitive complexity over time," helping quantify technical debt in a familiar, managerial-friendly way.

By following these patterns, Cogni's interface will feel familiar to stakeholders and integrate smoothly into the software development process, from local development through continuous integration and on to high-level reporting. The key is to **make Cogni's insights as accessible as code coverage or test results**, so that improving code understandability becomes a natural part of the development workflow.

**Sources:**

- Ned Batchelder, _Coverage.py Documentation_ - illustrating CLI usage and multiple output formats[\[23\]](https://coverage.readthedocs.io/en/7.13.1/#:~:text=,what%20tests%20ran%20which%20lines)[\[4\]](https://coverage.readthedocs.io/en/7.13.1/#:~:text=4,HTML%20listings%20detailing%20missed%20lines).
- Coala Documentation - example of a language-independent analysis CLI and config file usage[\[1\]](https://coala.github.io/website-old/#:~:text=coala%20provides%20a%20user%20and,and%20users%20focus%20on%20content)[\[14\]](https://coala.github.io/website-old/#:~:text=%60cd%20project%20%26%26%20coala%20,save)[\[18\]](https://coala.github.io/website-old/#:~:text=Just%20Execute%20).
- _StackScale Blog (2023)_ - Programming language popularity statistics[\[24\]](https://www.stackscale.com/blog/most-popular-programming-languages/#:~:text=Position%20PYPL%20ranking%20September%202023,9%20Swift%20C).
- Medium/CLI UX Guides - best practices for CLI design (structured output, task-focused commands)[\[7\]](https://medium.com/@kaushalsinh73/10-cli-ux-patterns-that-users-will-brag-about-015e4d0c268d#:~:text=7%29%20Structured%20Output%2C%20Stable%20Tables,by%20default%20acme%20services)[\[5\]](https://medium.com/@kaushalsinh73/10-cli-ux-patterns-that-users-will-brag-about-015e4d0c268d#:~:text=1,APIs).
- Bencher.dev - continuous benchmarking in CI (JSON outputs and trend tracking)[\[8\]](https://bencher.dev/learn/track-in-ci/cpp/google-benchmark/#:~:text=The%20C%2B%2B%20Google%20Adapter%20%28,are%20collected)[\[22\]](https://bencher.dev/learn/track-in-ci/cpp/google-benchmark/#:~:text=,they%20make%20it%20to%20production).
- Lucas F. Costa, _UX Patterns for CLI Tools_ - advice on output formatting and usability in terminal apps[\[10\]](https://www.lucasfcosta.com/blog/ux-patterns-cli-tools#:~:text=1,users%20may%20want%20to%20search)[\[13\]](https://www.lucasfcosta.com/blog/ux-patterns-cli-tools#:~:text=Exit%20codes).

[\[1\]](https://coala.github.io/website-old/#:~:text=coala%20provides%20a%20user%20and,and%20users%20focus%20on%20content) [\[14\]](https://coala.github.io/website-old/#:~:text=%60cd%20project%20%26%26%20coala%20,save) [\[18\]](https://coala.github.io/website-old/#:~:text=Just%20Execute%20) [\[20\]](https://coala.github.io/website-old/#:~:text=Alternative%3A%20Run%20it%20in%20Docker) coala - Code Analysis Made Easy

<https://coala.github.io/website-old/>

[\[2\]](https://www.stackscale.com/blog/most-popular-programming-languages/#:~:text=Position%20PYPL%20ranking%20September%202023,R%20Java%208%20TypeScript%20C) [\[24\]](https://www.stackscale.com/blog/most-popular-programming-languages/#:~:text=Position%20PYPL%20ranking%20September%202023,9%20Swift%20C) Most popular programming languages in 2023 \[ranking\]

<https://www.stackscale.com/blog/most-popular-programming-languages/>

[\[3\]](https://coverage.readthedocs.io/en/7.13.1/#:~:text=3.%20Use%20,report%20on%20the%20results) [\[4\]](https://coverage.readthedocs.io/en/7.13.1/#:~:text=4,HTML%20listings%20detailing%20missed%20lines) [\[6\]](https://coverage.readthedocs.io/en/7.13.1/#:~:text=To%20limit%20coverage%20measurement%20to,measure%20%20or%20%2030) [\[23\]](https://coverage.readthedocs.io/en/7.13.1/#:~:text=,what%20tests%20ran%20which%20lines) Coverage.py - Coverage.py 7.13.1 documentation

<https://coverage.readthedocs.io/en/7.13.1/>

[\[5\]](https://medium.com/@kaushalsinh73/10-cli-ux-patterns-that-users-will-brag-about-015e4d0c268d#:~:text=1,APIs) [\[7\]](https://medium.com/@kaushalsinh73/10-cli-ux-patterns-that-users-will-brag-about-015e4d0c268d#:~:text=7%29%20Structured%20Output%2C%20Stable%20Tables,by%20default%20acme%20services) [\[17\]](https://medium.com/@kaushalsinh73/10-cli-ux-patterns-that-users-will-brag-about-015e4d0c268d#:~:text=Ten%20CLI%20UX%20patterns%20%E2%80%94,%E2%80%94%20that%20make%20users%20rave) 10 CLI UX Patterns That Users Will Brag About | by Neurobyte | Medium

<https://medium.com/@kaushalsinh73/10-cli-ux-patterns-that-users-will-brag-about-015e4d0c268d>

[\[8\]](https://bencher.dev/learn/track-in-ci/cpp/google-benchmark/#:~:text=The%20C%2B%2B%20Google%20Adapter%20%28,are%20collected) [\[22\]](https://bencher.dev/learn/track-in-ci/cpp/google-benchmark/#:~:text=,they%20make%20it%20to%20production) How to track C++ Google Benchmark benchmarks in CI | Bencher - Continuous Benchmarking

<https://bencher.dev/learn/track-in-ci/cpp/google-benchmark/>

[\[9\]](https://www.lucasfcosta.com/blog/ux-patterns-cli-tools#:~:text=When%20the%20output%20is%20displayed,went%20okay%20despite%20the%20warning) [\[10\]](https://www.lucasfcosta.com/blog/ux-patterns-cli-tools#:~:text=1,users%20may%20want%20to%20search) [\[11\]](https://www.lucasfcosta.com/blog/ux-patterns-cli-tools#:~:text=1,users%20may%20want%20to%20search) [\[12\]](https://www.lucasfcosta.com/blog/ux-patterns-cli-tools#:~:text=One%20of%20my%20favourite%20progress,indicators%20is%20Docker%27s) [\[13\]](https://www.lucasfcosta.com/blog/ux-patterns-cli-tools#:~:text=Exit%20codes) UX patterns for CLI tools

<https://www.lucasfcosta.com/blog/ux-patterns-cli-tools>

[\[15\]](https://thedocumentation.org/super-linter/usage/configuration/#:~:text=Configuration%20via%20Environment%20Variables) [\[16\]](https://thedocumentation.org/super-linter/usage/configuration/#:~:text=The%20%60VALIDATE_,specific%20logic) Configuration - Usage Guide - super-linter Documentation

<https://thedocumentation.org/super-linter/usage/configuration/>

[\[19\]](https://github.com/marketplace/actions/codecov#:~:text=Codecov%20%C2%B7%20Actions%20%C2%B7%20GitHub,Wrapper%20to%20encapsulate%20the%20CLI) Codecov · Actions · GitHub Marketplace

<https://github.com/marketplace/actions/codecov>

[\[21\]](https://stackoverflow.com/questions/46117069/how-to-force-black-and-white-output-in-google-benchmark#:~:text=The%20format%20of%20the%20output,does%20not%20suppress%20the) How to force black and white output in google benchmark

<https://stackoverflow.com/questions/46117069/how-to-force-black-and-white-output-in-google-benchmark>
